{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed7750a-cc36-4072-b2c0-284bf8f53473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "from openai import OpenAI\n",
    "import dspy\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2e36ed-f6af-4962-b69a-aee89f995f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate correctness of the answer - depends on format - will we pass two integers here as proposed by Vineet's logic?\n",
    "def evaluate_answer(model_answer, correct_answer):\n",
    "    if model_answer == \"A\" and correct_answer == 0:\n",
    "      return True\n",
    "    elif model_answer == \"B\" and correct_answer == 1:\n",
    "      return True\n",
    "    elif model_answer == \"C\" and correct_answer == 2:\n",
    "      return True\n",
    "    elif model_answer == \"D\" and correct_answer == 3:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "726a25f5-7d18-443c-a51f-d4b77bf3dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_df=pd.read_csv('train_df.csv')\n",
    "train_df=train_df.fillna('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df4dbd4-8e08-4235-bae4-9c07e91ca1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.OpenAI(model='gpt-4o-mini', api_key='<add_your_key_here>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9ca327-5452-4a1d-9ffb-416c8c35756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9c192-969b-45bc-8807-580e79451f9c",
   "metadata": {},
   "source": [
    "### Methods for confidence scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc01b29b-cf52-477f-b349-2ab8642a142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_mc_question_1hop(context, question, choices):\n",
    "    '''\n",
    "    Ask LLM to respond with correct answer and associated confidence (only for correct option)\n",
    "    '''\n",
    "    # Format the choices - using characters for now - maybe change to integers?\n",
    "    choices_text = '\\n'.join([f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)])\n",
    "\n",
    "    # Construct the prompt\n",
    "    if context!='#':\n",
    "        prompt = f\"\"\"\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Options: {choices_text}\n",
    "        Please choose the best answer by returning the letter (A, B, C, etc.) corresponding to the correct choice.  Just the letter, nothing else.\n",
    "        Provide the probability that your guess is correct. Give ONLY the probability, no\n",
    "        other words or explanation.\n",
    "        Finally return a tuple (choice, probability) where probability is between 0 and 1.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Options: {choices_text}\n",
    "        Please choose the best answer by returning the letter (A, B, C, etc.) corresponding to the correct choice.  Just the letter, nothing else.\n",
    "        Provide the probability that your guess is correct. Give ONLY the probability, no\n",
    "        other words or explanation.\n",
    "        Finally return a tuple (choice, probability) where probability is between 0 and 1.\n",
    "        \"\"\"\n",
    "\n",
    "    # Call the OpenAI API with the constructed prompt\n",
    "    response = lm(prompt, temperature=0, n=1)\n",
    "\n",
    "    # print(f\"Prompt: {prompt}\")\n",
    "    # print(f\"Response: {response}\")\n",
    "    response=response[0]\n",
    "    # Check if there's a comma in the response for splitting\n",
    "    if ',' in response:\n",
    "        answer, conf = response.split(',')\n",
    "    else:\n",
    "        answer = response\n",
    "        conf = 0.0  # or set to a default value like 0.0\n",
    "    # answer,conf = response[0].split(',')\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    # print(f\"Confidence: {conf}\")\n",
    "\n",
    "    return answer,conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "776ee199-b4df-41b2-9e20-bac1894770dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(C', ' 0.8)')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_context = \"Some Cantonese don't like chili, so some southerners don't like chili.\"\n",
    "test_question = \"Which of the following can guarantee the above argument?\"\n",
    "test_choices = [\"Some Cantonese love chili.\", \"Some people who like peppers are southerners.\", \"All Cantonese are southerners.\", \"Some Cantonese like neither peppers nor sweets.\"]\n",
    "test_correct_answer = 2\n",
    "\n",
    "# Get the LLM's answer\n",
    "ask_llm_mc_question_1hop(test_context, test_question, test_choices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bc2b4-9fde-4ab3-875b-0baf269f00d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4198566a-5216-4068-809d-aa95ef5978dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_mc_question_1hop_withconf(context, question, choices):\n",
    "    '''\n",
    "    Ask LLM to respond with  confidence for all  options\n",
    "    '''\n",
    "    # Format the choices - using characters for now - maybe change to integers?\n",
    "    choices_text = '\\n'.join([f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)])\n",
    "\n",
    "    # Construct the prompt\n",
    "\n",
    "    if context!='#':\n",
    "        prompt = f\"\"\"\n",
    "        Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Options: {choices_text}\n",
    "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
    "        Question: {question}\n",
    "        Options: {choices_text}\n",
    "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the OpenAI API with the constructed prompt\n",
    "    response = lm(prompt, temperature=0, n=1)\n",
    "\n",
    "    # print(f\"Prompt: {prompt}\")\n",
    "    # print(f\"Response: {response}\")\n",
    "    # print(response)\n",
    "    # conf = response[0].split(',')\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    # print(f\"Confidence: {conf}\")\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7d97f-7ee0-487d-b87f-13b334a8ecbe",
   "metadata": {},
   "source": [
    "### Iterate over dataset and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f34ec-1ba6-4c15-b765-31a1a611b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "commonsense_qa_trainset_results_conf={}\n",
    "for idx,row in tqdm(train_df[train_df['ds_name']=='commonsense_qa'].iterrows()):\n",
    "    answer=ask_llm_mc_question_1hop_withconf(row['context'], row['question'], ast.literal_eval(row['options']))\n",
    "    commonsense_qa_trainset_results_conf[idx]=(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad6e63-57b5-4b5c-afd2-d405eb5bc147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b0a74-fd7f-40da-9da5-c8d02d2db07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892eeb9f-2b5a-442a-9acb-f3eceef3bc27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72feae-04b4-401f-b487-3b0f41ae717f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e3716d-cd2b-4644-a685-b92357e5a277",
   "metadata": {},
   "source": [
    "## TODO: Logit based expt \n",
    "see here : https://cookbook.openai.com/examples/using_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3118a541-a677-497b-80f2-5c5816f30d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from math import exp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3218cd3b-4288-4f0c-a34b-217731b6e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='<add_key_here>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dda955f2-d063-4f92-852e-9c9d7d9e7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a98f74fb-85b1-4394-852b-ef370cd0cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = \"\"\"Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Options: {choices_text}\n",
    "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57044607-0e95-4ae1-84f0-4ba67a4686c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"Some Cantonese don't like chili, so some southerners don't like chili.\"\n",
    "test_question = \"Which of the following can guarantee the above argument?\"\n",
    "test_choices = [\"Some Cantonese love chili.\", \"Some people who like peppers are southerners.\", \"All Cantonese are southerners.\", \"Some Cantonese like neither peppers nor sweets.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c361cc5a-387e-47aa-ae72-25aaad8a03e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TopLogprob(token='[', bytes=[91], logprob=-0.01817628), TopLogprob(token='0', bytes=[48], logprob=-4.268176), TopLogprob(token=\"['\", bytes=[91, 39], logprob=-5.643176), TopLogprob(token='1', bytes=[49], logprob=-7.893176), TopLogprob(token='-', bytes=[45], logprob=-10.143176), TopLogprob(token='[\\n', bytes=[91, 10], logprob=-11.393176), TopLogprob(token='Prob', bytes=[80, 114, 111, 98], logprob=-11.393176), TopLogprob(token='Here', bytes=[72, 101, 114, 101], logprob=-11.643176), TopLogprob(token='Probability', bytes=[80, 114, 111, 98, 97, 98, 105, 108, 105, 116, 121], logprob=-12.393176), TopLogprob(token='Option', bytes=[79, 112, 116, 105, 111, 110], logprob=-12.518176)]\n"
     ]
    }
   ],
   "source": [
    "API_RESPONSE = get_completion(\n",
    "    [{\"role\": \"user\", \"content\": QUERY_PROMPT.format(context=test_context,question=test_question,choices_text=test_choices)}],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    logprobs=True,\n",
    "    top_logprobs=10,\n",
    ")\n",
    "top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n",
    "\n",
    "print(top_two_logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9113179-259a-465f-a665-3194a6f30fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> [, <span style='color: darkorange'>logprobs:</span> -0.01817628, <span style='color: magenta'>linear probability:</span> 98.2%<br><span style='color: cyan'>Output token 2:</span> 0, <span style='color: darkorange'>logprobs:</span> -4.268176, <span style='color: magenta'>linear probability:</span> 1.4%<br><span style='color: cyan'>Output token 3:</span> [', <span style='color: darkorange'>logprobs:</span> -5.643176, <span style='color: magenta'>linear probability:</span> 0.35%<br><span style='color: cyan'>Output token 4:</span> 1, <span style='color: darkorange'>logprobs:</span> -7.893176, <span style='color: magenta'>linear probability:</span> 0.04%<br><span style='color: cyan'>Output token 5:</span> -, <span style='color: darkorange'>logprobs:</span> -10.143176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 6:</span> [\n",
       ", <span style='color: darkorange'>logprobs:</span> -11.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 7:</span> Prob, <span style='color: darkorange'>logprobs:</span> -11.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 8:</span> Here, <span style='color: darkorange'>logprobs:</span> -11.643176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 9:</span> Probability, <span style='color: darkorange'>logprobs:</span> -12.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 10:</span> Option, <span style='color: darkorange'>logprobs:</span> -12.518176, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_content = \"\"\n",
    "for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "    html_content += (\n",
    "        f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
    "        f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
    "        f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n",
    "    )\n",
    "display(HTML(html_content))\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbd625-fc05-44ec-9cfc-852fc95fa84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8f5f8-19e1-4a91-ab56-cbdbef54fb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ae5c1-5d88-4aee-b0af-4df98b55b9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5ecb6-9f45-4c26-b101-8284a5994978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcf1be-f50e-4303-af55-a95889031881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09d22f-bd4c-4780-8182-45e4066f6fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd1849-c1aa-4068-9332-b7885255c16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb47b5-5e52-45bc-935e-8bb5ce1b0090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220cacb7-a187-47dd-95ce-f879f85328c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50804712-5fe2-4b4d-ae49-379f8a8e40e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8419550-b762-4d79-83d6-45ee1dbcf76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad8f14-6083-4eca-9d84-a253e5b35be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35eea1-645a-4700-bb15-ba8281118000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916905f8-e67b-4963-b499-7b0a82e5cf21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdd9e2-56a8-44c9-806e-acd74d520c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b37d82-c85b-4706-81a5-c88b6f6d83b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0469c11-2021-4af8-a533-ef79406b408d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa099a-68aa-4797-bd80-27e3ba931d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d35dd5-c063-4fe7-bda6-a3e8b654f095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73b6a4-a5aa-467c-8cb6-bfdd3ab2b10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5944c90-22c6-4ae4-b347-68f57fb5c223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99162024-2041-477c-bcd4-61a78c5f9391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec01fb5-7a72-489b-8fe7-ad1b2ba83726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423c221-a913-42a0-aee7-064cc94f373e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e99bd8-ef2d-481f-a298-3cb2754a965a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5037807-429e-4f9d-a0ef-d1f72a43885e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf7b2c-9ad9-4f65-984c-f0103ee2f333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c29643-2fdc-493d-89a1-8a70959c0fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5f73b-1f05-49eb-92f8-8e4ad6d63a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
