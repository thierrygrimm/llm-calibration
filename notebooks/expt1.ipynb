{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Set-Up"
      ],
      "metadata": {
        "id": "ovFCTTS_r9BZ"
      },
      "id": "ovFCTTS_r9BZ"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9ed7750a-cc36-4072-b2c0-284bf8f53473",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ed7750a-cc36-4072-b2c0-284bf8f53473",
        "outputId": "26d8395e-c303-48a1-bf8d-f85300ecdc9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy\n",
            "  Downloading dspy-2.5.20-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from dspy)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting datasets (from dspy)\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.10/dist-packages (from dspy) (1.4.2)\n",
            "Collecting litellm<=1.49.1 (from dspy)\n",
            "  Downloading litellm-1.49.1-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from dspy) (1.52.2)\n",
            "Collecting optuna (from dspy)\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dspy) (2.2.2)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.10/dist-packages (from dspy) (2.9.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from dspy) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dspy) (2.32.3)\n",
            "Collecting structlog (from dspy)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dspy) (4.66.5)\n",
            "Collecting ujson (from dspy)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from dspy) (0.27.2)\n",
            "Collecting magicattr~=0.1.6 (from dspy)\n",
            "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting diskcache (from dspy)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting json-repair (from dspy)\n",
            "  Downloading json_repair-0.30.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm<=1.49.1->dspy) (3.10.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm<=1.49.1->dspy) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm<=1.49.1->dspy) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<=1.49.1->dspy) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm<=1.49.1->dspy) (4.23.0)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm<=1.49.1->dspy)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tiktoken>=0.7.0 (from litellm<=1.49.1->dspy)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm<=1.49.1->dspy) (0.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->dspy) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai->dspy) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->dspy) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai->dspy) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->dspy) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->dspy) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->dspy) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->dspy) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic~=2.0->dspy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic~=2.0->dspy) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dspy) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dspy) (2.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->dspy) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->dspy) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->dspy) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->dspy)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->dspy)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->dspy)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->dspy) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->dspy) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->dspy) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->dspy) (6.0.2)\n",
            "Collecting alembic>=1.5.0 (from optuna->dspy)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna->dspy)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->dspy) (2.0.36)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy) (2024.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->dspy)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->dspy) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm<=1.49.1->dspy) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm<=1.49.1->dspy) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm<=1.49.1->dspy) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm<=1.49.1->dspy) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm<=1.49.1->dspy) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm<=1.49.1->dspy) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm<=1.49.1->dspy) (4.0.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm<=1.49.1->dspy) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<=1.49.1->dspy) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<=1.49.1->dspy) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<=1.49.1->dspy) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<=1.49.1->dspy) (0.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dspy) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->dspy) (3.1.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->litellm<=1.49.1->dspy) (0.2.0)\n",
            "Downloading dspy-2.5.20-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.49.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.30.0-py3-none-any.whl (17 kB)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: magicattr, xxhash, ujson, structlog, python-dotenv, Mako, json-repair, diskcache, dill, colorlog, backoff, tiktoken, multiprocess, alembic, optuna, litellm, datasets, dspy\n",
            "Successfully installed Mako-1.3.6 alembic-1.13.3 backoff-2.2.1 colorlog-6.9.0 datasets-3.0.2 dill-0.3.8 diskcache-5.6.3 dspy-2.5.20 json-repair-0.30.0 litellm-1.49.1 magicattr-0.1.6 multiprocess-0.70.16 optuna-4.0.0 python-dotenv-1.0.1 structlog-24.4.0 tiktoken-0.8.0 ujson-5.10.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install openai\n",
        "!pip install dspy\n",
        "from openai import OpenAI\n",
        "import dspy\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fa2e36ed-f6af-4962-b69a-aee89f995f3e",
      "metadata": {
        "id": "fa2e36ed-f6af-4962-b69a-aee89f995f3e"
      },
      "outputs": [],
      "source": [
        "# Evaluate correctness of the answer - depends on format - will we pass two integers here as proposed by Vineet's logic?\n",
        "def evaluate_answer(model_answer, correct_answer):\n",
        "    if model_answer == \"A\" and correct_answer == 0:\n",
        "      return True\n",
        "    elif model_answer == \"B\" and correct_answer == 1:\n",
        "      return True\n",
        "    elif model_answer == \"C\" and correct_answer == 2:\n",
        "      return True\n",
        "    elif model_answer == \"D\" and correct_answer == 3:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "726a25f5-7d18-443c-a51f-d4b77bf3dee2",
      "metadata": {
        "id": "726a25f5-7d18-443c-a51f-d4b77bf3dee2"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "train_df=pd.read_csv('train_df.csv')\n",
        "train_df=train_df.fillna('#')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key='<add_your_key_here>'\n",
        "#Pull the OpenAI key from colab\n",
        "from google.colab import userdata\n",
        "openai_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "4TBUVUI8uv1U"
      },
      "id": "4TBUVUI8uv1U",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1df4dbd4-8e08-4235-bae4-9c07e91ca1c2",
      "metadata": {
        "id": "1df4dbd4-8e08-4235-bae4-9c07e91ca1c2"
      },
      "outputs": [],
      "source": [
        "lm = dspy.OpenAI(model='gpt-4o-mini', api_key=openai_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7a9ca327-5452-4a1d-9ffb-416c8c35756a",
      "metadata": {
        "id": "7a9ca327-5452-4a1d-9ffb-416c8c35756a"
      },
      "outputs": [],
      "source": [
        "dspy.settings.configure(lm=lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Questions"
      ],
      "metadata": {
        "id": "hgXI_rPTsQXX"
      },
      "id": "hgXI_rPTsQXX"
    },
    {
      "cell_type": "code",
      "source": [
        "#Question id=0 in logiqa train data set.  Use for debugging prompts.\n",
        "test_context = \"Some Cantonese don't like chili, so some southerners don't like chili.\"\n",
        "test_question = \"Which of the following can guarantee the above argument?\"\n",
        "test_choices = [\"Some Cantonese love chili.\", \"Some people who like peppers are southerners.\", \"All Cantonese are southerners.\", \"Some Cantonese like neither peppers nor sweets.\"]\n",
        "test_answer_probabilities = [0.05,0.10,0.80,0.05]\n",
        "test_answer_letter_choice = 'C'\n",
        "test_correct_answer = 2\n",
        "\n",
        "#Note that this is Question id=1 in logiqa train data set.  Need to remove it from train data set?\n",
        "fewshot_example1_context = \"Continuous exposure to indoor fluorescent lights is beneficial to the health of hamsters with heart disease. One group of hamsters exposed to continuous exposure to fluorescent lights has an average lifespan that is 2.5% longer than another one of the same species but living in a black wall.\"\n",
        "fewshot_example1_question = \"Which of the following questions was the initial motivation for conducting the above experiment?\"\n",
        "fewshot_example1_choices = ['Can hospital light therapy be proved to promote patient recovery?', 'Which one lives longer, the hamster living under the light or the hamster living in the dark?', 'What kind of illness does the hamster have?', 'Do some hamsters need a period of darkness?']\n",
        "fewshot_example1_answer_probabilities = [0.75,0.10,0.02,0.13]\n",
        "fewshot_example1_answer_letter_choice = 'A'\n",
        "fewshot_example1_correct_answer = 0"
      ],
      "metadata": {
        "id": "vKjG7D3bsQoG"
      },
      "id": "vKjG7D3bsQoG",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d0b9c192-969b-45bc-8807-580e79451f9c",
      "metadata": {
        "id": "d0b9c192-969b-45bc-8807-580e79451f9c"
      },
      "source": [
        "### Prompts and LLM Query"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_choices(choices):\n",
        "    # Format the choices - using characters for now - maybe change to integers?\n",
        "    choices_text = '\\n'.join([f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)])\n",
        "    return choices_text\n",
        ""
      ],
      "metadata": {
        "id": "LyYPteDE2pUm"
      },
      "id": "LyYPteDE2pUm",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "dc01b29b-cf52-477f-b349-2ab8642a142d",
      "metadata": {
        "id": "dc01b29b-cf52-477f-b349-2ab8642a142d"
      },
      "outputs": [],
      "source": [
        "def ask_llm_mc_question_1hop(context, question, choices, debug=False):\n",
        "    '''\n",
        "    Ask LLM to respond with correct answer and associated confidence (only for correct option)\n",
        "    '''\n",
        "    choices_text = format_choices(choices)\n",
        "\n",
        "    # Construct the prompt\n",
        "    if context!='#':\n",
        "        prompt = f\"\"\"\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Please choose the best answer by returning the letter (A, B, C, etc.) corresponding to the correct choice.  Just the letter, nothing else.\n",
        "        Provide the probability that your guess is correct. Give ONLY the probability, no\n",
        "        other words or explanation.\n",
        "        Finally return a tuple (choice, probability) where probability is between 0 and 1.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Please choose the best answer by returning the letter (A, B, C, etc.) corresponding to the correct choice.  Just the letter, nothing else.\n",
        "        Provide the probability that your guess is correct. Give ONLY the probability, no\n",
        "        other words or explanation.\n",
        "        Finally return a tuple (choice, probability) where probability is between 0 and 1.\n",
        "        \"\"\"\n",
        "\n",
        "    # Call the OpenAI API with the constructed prompt\n",
        "    response = lm(prompt, temperature=0, n=1)\n",
        "\n",
        "    response=response[0]\n",
        "    # Check if there's a comma in the response for splitting\n",
        "    if ',' in response:\n",
        "        answer, conf = response.split(',')\n",
        "    else:\n",
        "        answer = response\n",
        "        conf = 0.0  # or set to a default value like 0.0\n",
        "    # answer,conf = response[0].split(',')\n",
        "    if debug:\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {response}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(f\"Confidence: {conf}\")\n",
        "\n",
        "    return answer,conf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "776ee199-b4df-41b2-9e20-bac1894770dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "776ee199-b4df-41b2-9e20-bac1894770dd",
        "outputId": "7c439496-1577-4cdd-e2a6-75ecb7a9c559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \n",
            "        Context: Some Cantonese don't like chili, so some southerners don't like chili.\n",
            "        Question: Which of the following can guarantee the above argument?\n",
            "        Options: A. Some Cantonese love chili.\n",
            "B. Some people who like peppers are southerners.\n",
            "C. All Cantonese are southerners.\n",
            "D. Some Cantonese like neither peppers nor sweets.\n",
            "        Please choose the best answer by returning the letter (A, B, C, etc.) corresponding to the correct choice.  Just the letter, nothing else.\n",
            "        Provide the probability that your guess is correct. Give ONLY the probability, no\n",
            "        other words or explanation.\n",
            "        Finally return a tuple (choice, probability) where probability is between 0 and 1.\n",
            "        \n",
            "Response: (C, 0.8)\n",
            "Answer: (C\n",
            "Confidence:  0.8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('(C', ' 0.8)')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Get the LLM's answer\n",
        "ask_llm_mc_question_1hop(test_context, test_question, test_choices, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4198566a-5216-4068-809d-aa95ef5978dd",
      "metadata": {
        "id": "4198566a-5216-4068-809d-aa95ef5978dd"
      },
      "outputs": [],
      "source": [
        "def ask_llm_mc_question_1hop_withconf(context, question, choices, debug=False):\n",
        "    '''\n",
        "    Ask LLM to respond with  confidence for all  options\n",
        "    '''\n",
        "    choices_text = format_choices(choices)\n",
        "\n",
        "    # Construct the prompt\n",
        "\n",
        "    if context!='#':\n",
        "        prompt = f\"\"\"\n",
        "        Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"\n",
        "        Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the OpenAI API with the constructed prompt\n",
        "    response = lm(prompt, temperature=0, n=1)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the LLM's answer\n",
        "ask_llm_mc_question_1hop_withconf(test_context, test_question, test_choices, debug=True)"
      ],
      "metadata": {
        "id": "87yzO86jxc8d",
        "outputId": "c5160ed4-232f-4411-f24f-86eedd6a000d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "87yzO86jxc8d",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \n",
            "        Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
            "        Context: Some Cantonese don't like chili, so some southerners don't like chili.\n",
            "        Question: Which of the following can guarantee the above argument?\n",
            "        Options: A. Some Cantonese love chili.\n",
            "B. Some people who like peppers are southerners.\n",
            "C. All Cantonese are southerners.\n",
            "D. Some Cantonese like neither peppers nor sweets.\n",
            "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
            "        \n",
            "Response: ['A. 0  \\nB. 0  \\nC. 1  \\nD. 0  ']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A. 0  \\nB. 0  \\nC. 1  \\nD. 0  ']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "17ad6e63-57b5-4b5c-afd2-d405eb5bc147",
      "metadata": {
        "id": "17ad6e63-57b5-4b5c-afd2-d405eb5bc147"
      },
      "outputs": [],
      "source": [
        "def ask_llm_mc_question_1hop_withconf_fewshot(context, question, choices, fewshot=True, debug=False):\n",
        "    '''\n",
        "    Ask LLM to respond with  confidence for all  options, using few-shot prompting\n",
        "    '''\n",
        "\n",
        "    choices_text = format_choices(choices)\n",
        "    fewshot_example1_choices_text = format_choices(fewshot_example1_choices)\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt_task = \"Task: Provide the probability (between 0 and 1) for each option being correct, corresponding to options A, B, C, and D.  Return exactly 4 probabilities. No explanation is needed, only the probabilities.\"\n",
        "\n",
        "    if fewshot:\n",
        "        prompt_examples = f\"\"\"\n",
        "        Example 1:\n",
        "        Context: {fewshot_example1_context}\n",
        "        Question: {fewshot_example1_question}\n",
        "        Options: {fewshot_example1_choices_text}\n",
        "        Answer: {fewshot_example1_answer_probabilities}\n",
        "        \"\"\"\n",
        "        ##Example 2:\n",
        "        ##Context: {context}\n",
        "        ##Question: {question}\n",
        "        ##Options: {choices_text}\n",
        "        ##Answer: {answer}\n",
        "        ##\"\"\"\n",
        "    else:\n",
        "        prompt_examples = \"\"\n",
        "\n",
        "    if context!='#':\n",
        "        prompt_actual_question = f\"\"\"\n",
        "        Actual Question:\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt_actual_question = f\"\"\"\n",
        "        Actual Question:\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Answer:\n",
        "    \"\"\"\n",
        "    prompt = prompt_task + prompt_examples + prompt_actual_question\n",
        "\n",
        "    # Call the OpenAI API with the constructed prompt\n",
        "    response = lm(prompt, temperature=0, n=1)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the LLM's answer\n",
        "ask_llm_mc_question_1hop_withconf_fewshot(test_context, test_question, test_choices, fewshot=True, debug=True)"
      ],
      "metadata": {
        "id": "HRRBMomw1RfH",
        "outputId": "371f23c5-3b9d-4ac4-ef1b-39b34f007cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HRRBMomw1RfH",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Task: Provide the probability (between 0 and 1) for each option being correct, corresponding to options A, B, C, and D.  Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
            "        Example 1:\n",
            "        Context: Continuous exposure to indoor fluorescent lights is beneficial to the health of hamsters with heart disease. One group of hamsters exposed to continuous exposure to fluorescent lights has an average lifespan that is 2.5% longer than another one of the same species but living in a black wall.\n",
            "        Question: Which of the following questions was the initial motivation for conducting the above experiment?\n",
            "        Options: A. Can hospital light therapy be proved to promote patient recovery?\n",
            "B. Which one lives longer, the hamster living under the light or the hamster living in the dark?\n",
            "C. What kind of illness does the hamster have?\n",
            "D. Do some hamsters need a period of darkness?\n",
            "        Answer: [0.75, 0.1, 0.02, 0.13]\n",
            "        \n",
            "        Actual Question:\n",
            "        Context: Some Cantonese don't like chili, so some southerners don't like chili.\n",
            "        Question: Which of the following can guarantee the above argument?\n",
            "        Options: A. Some Cantonese love chili.\n",
            "B. Some people who like peppers are southerners.\n",
            "C. All Cantonese are southerners.\n",
            "D. Some Cantonese like neither peppers nor sweets.\n",
            "        Answer:\n",
            "        \n",
            "Response: ['[0.1, 0.2, 0.6, 0.1]']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[0.1, 0.2, 0.6, 0.1]']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc7d97f-7ee0-487d-b87f-13b334a8ecbe",
      "metadata": {
        "id": "9bc7d97f-7ee0-487d-b87f-13b334a8ecbe"
      },
      "source": [
        "### Iterate over dataset and store results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317f34ec-1ba6-4c15-b765-31a1a611b5d2",
      "metadata": {
        "id": "317f34ec-1ba6-4c15-b765-31a1a611b5d2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import ast\n",
        "\n",
        "commonsense_qa_trainset_results_conf={}\n",
        "for idx,row in tqdm(train_df[train_df['ds_name']=='commonsense_qa'].iterrows()):\n",
        "    answer=ask_llm_mc_question_1hop_withconf(row['context'], row['question'], ast.literal_eval(row['options']))\n",
        "    commonsense_qa_trainset_results_conf[idx]=(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sgh-cZfOqLEH"
      },
      "id": "Sgh-cZfOqLEH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837b0a74-fd7f-40da-9da5-c8d02d2db07f",
      "metadata": {
        "id": "837b0a74-fd7f-40da-9da5-c8d02d2db07f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892eeb9f-2b5a-442a-9acb-f3eceef3bc27",
      "metadata": {
        "id": "892eeb9f-2b5a-442a-9acb-f3eceef3bc27"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e72feae-04b4-401f-b487-3b0f41ae717f",
      "metadata": {
        "id": "9e72feae-04b4-401f-b487-3b0f41ae717f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e4e3716d-cd2b-4644-a685-b92357e5a277",
      "metadata": {
        "id": "e4e3716d-cd2b-4644-a685-b92357e5a277"
      },
      "source": [
        "## TODO: Logit based expt\n",
        "see here : https://cookbook.openai.com/examples/using_logprobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3118a541-a677-497b-80f2-5c5816f30d0b",
      "metadata": {
        "id": "3118a541-a677-497b-80f2-5c5816f30d0b"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from math import exp\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3218cd3b-4288-4f0c-a34b-217731b6e8b0",
      "metadata": {
        "id": "3218cd3b-4288-4f0c-a34b-217731b6e8b0"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key='<add_key_here>')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda955f2-d063-4f92-852e-9c9d7d9e7d9d",
      "metadata": {
        "id": "dda955f2-d063-4f92-852e-9c9d7d9e7d9d"
      },
      "outputs": [],
      "source": [
        "def get_completion(\n",
        "    messages: list[dict[str, str]],\n",
        "    model: str = \"gpt-4o-mini\",\n",
        "    max_tokens=500,\n",
        "    temperature=0,\n",
        "    stop=None,\n",
        "    seed=123,\n",
        "    tools=None,\n",
        "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
        "    top_logprobs=None,\n",
        ") -> str:\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"stop\": stop,\n",
        "        \"seed\": seed,\n",
        "        \"logprobs\": logprobs,\n",
        "        \"top_logprobs\": top_logprobs,\n",
        "    }\n",
        "    if tools:\n",
        "        params[\"tools\"] = tools\n",
        "\n",
        "    completion = client.chat.completions.create(**params)\n",
        "    return completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98f74fb-85b1-4394-852b-ef370cd0cf2a",
      "metadata": {
        "id": "a98f74fb-85b1-4394-852b-ef370cd0cf2a"
      },
      "outputs": [],
      "source": [
        "QUERY_PROMPT = \"\"\"Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57044607-0e95-4ae1-84f0-4ba67a4686c5",
      "metadata": {
        "id": "57044607-0e95-4ae1-84f0-4ba67a4686c5"
      },
      "outputs": [],
      "source": [
        "test_context = \"Some Cantonese don't like chili, so some southerners don't like chili.\"\n",
        "test_question = \"Which of the following can guarantee the above argument?\"\n",
        "test_choices = [\"Some Cantonese love chili.\", \"Some people who like peppers are southerners.\", \"All Cantonese are southerners.\", \"Some Cantonese like neither peppers nor sweets.\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c361cc5a-387e-47aa-ae72-25aaad8a03e8",
      "metadata": {
        "id": "c361cc5a-387e-47aa-ae72-25aaad8a03e8",
        "outputId": "92796e10-633d-4130-8b6c-4a29107088e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TopLogprob(token='[', bytes=[91], logprob=-0.01817628), TopLogprob(token='0', bytes=[48], logprob=-4.268176), TopLogprob(token=\"['\", bytes=[91, 39], logprob=-5.643176), TopLogprob(token='1', bytes=[49], logprob=-7.893176), TopLogprob(token='-', bytes=[45], logprob=-10.143176), TopLogprob(token='[\\n', bytes=[91, 10], logprob=-11.393176), TopLogprob(token='Prob', bytes=[80, 114, 111, 98], logprob=-11.393176), TopLogprob(token='Here', bytes=[72, 101, 114, 101], logprob=-11.643176), TopLogprob(token='Probability', bytes=[80, 114, 111, 98, 97, 98, 105, 108, 105, 116, 121], logprob=-12.393176), TopLogprob(token='Option', bytes=[79, 112, 116, 105, 111, 110], logprob=-12.518176)]\n"
          ]
        }
      ],
      "source": [
        "API_RESPONSE = get_completion(\n",
        "    [{\"role\": \"user\", \"content\": QUERY_PROMPT.format(context=test_context,question=test_question,choices_text=test_choices)}],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    logprobs=True,\n",
        "    top_logprobs=10,\n",
        ")\n",
        "top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n",
        "\n",
        "print(top_two_logprobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9113179-259a-465f-a665-3194a6f30fdf",
      "metadata": {
        "id": "a9113179-259a-465f-a665-3194a6f30fdf",
        "outputId": "3ce38b98-07c6-4878-e403-4d7d9ffcb72c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span style='color: cyan'>Output token 1:</span> [, <span style='color: darkorange'>logprobs:</span> -0.01817628, <span style='color: magenta'>linear probability:</span> 98.2%<br><span style='color: cyan'>Output token 2:</span> 0, <span style='color: darkorange'>logprobs:</span> -4.268176, <span style='color: magenta'>linear probability:</span> 1.4%<br><span style='color: cyan'>Output token 3:</span> [', <span style='color: darkorange'>logprobs:</span> -5.643176, <span style='color: magenta'>linear probability:</span> 0.35%<br><span style='color: cyan'>Output token 4:</span> 1, <span style='color: darkorange'>logprobs:</span> -7.893176, <span style='color: magenta'>linear probability:</span> 0.04%<br><span style='color: cyan'>Output token 5:</span> -, <span style='color: darkorange'>logprobs:</span> -10.143176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 6:</span> [\n",
              ", <span style='color: darkorange'>logprobs:</span> -11.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 7:</span> Prob, <span style='color: darkorange'>logprobs:</span> -11.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 8:</span> Here, <span style='color: darkorange'>logprobs:</span> -11.643176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 9:</span> Probability, <span style='color: darkorange'>logprobs:</span> -12.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 10:</span> Option, <span style='color: darkorange'>logprobs:</span> -12.518176, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "html_content = \"\"\n",
        "for i, logprob in enumerate(top_two_logprobs, start=1):\n",
        "    html_content += (\n",
        "        f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
        "        f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
        "        f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n",
        "    )\n",
        "display(HTML(html_content))\n",
        "print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdbd625-fc05-44ec-9cfc-852fc95fa84e",
      "metadata": {
        "id": "4fdbd625-fc05-44ec-9cfc-852fc95fa84e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd8f5f8-19e1-4a91-ab56-cbdbef54fb1b",
      "metadata": {
        "id": "5fd8f5f8-19e1-4a91-ab56-cbdbef54fb1b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1ae5c1-5d88-4aee-b0af-4df98b55b9df",
      "metadata": {
        "id": "ad1ae5c1-5d88-4aee-b0af-4df98b55b9df"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c5ecb6-9f45-4c26-b101-8284a5994978",
      "metadata": {
        "id": "27c5ecb6-9f45-4c26-b101-8284a5994978"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fcf1be-f50e-4303-af55-a95889031881",
      "metadata": {
        "id": "06fcf1be-f50e-4303-af55-a95889031881"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f09d22f-bd4c-4780-8182-45e4066f6fcf",
      "metadata": {
        "id": "3f09d22f-bd4c-4780-8182-45e4066f6fcf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfd1849-c1aa-4068-9332-b7885255c16f",
      "metadata": {
        "id": "1dfd1849-c1aa-4068-9332-b7885255c16f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbb47b5-5e52-45bc-935e-8bb5ce1b0090",
      "metadata": {
        "id": "1dbb47b5-5e52-45bc-935e-8bb5ce1b0090"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220cacb7-a187-47dd-95ce-f879f85328c6",
      "metadata": {
        "id": "220cacb7-a187-47dd-95ce-f879f85328c6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50804712-5fe2-4b4d-ae49-379f8a8e40e6",
      "metadata": {
        "id": "50804712-5fe2-4b4d-ae49-379f8a8e40e6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8419550-b762-4d79-83d6-45ee1dbcf76b",
      "metadata": {
        "id": "f8419550-b762-4d79-83d6-45ee1dbcf76b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ad8f14-6083-4eca-9d84-a253e5b35be0",
      "metadata": {
        "id": "49ad8f14-6083-4eca-9d84-a253e5b35be0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab35eea1-645a-4700-bb15-ba8281118000",
      "metadata": {
        "id": "ab35eea1-645a-4700-bb15-ba8281118000"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916905f8-e67b-4963-b499-7b0a82e5cf21",
      "metadata": {
        "id": "916905f8-e67b-4963-b499-7b0a82e5cf21"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71cdd9e2-56a8-44c9-806e-acd74d520c54",
      "metadata": {
        "id": "71cdd9e2-56a8-44c9-806e-acd74d520c54"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b37d82-c85b-4706-81a5-c88b6f6d83b1",
      "metadata": {
        "id": "13b37d82-c85b-4706-81a5-c88b6f6d83b1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0469c11-2021-4af8-a533-ef79406b408d",
      "metadata": {
        "id": "a0469c11-2021-4af8-a533-ef79406b408d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31aa099a-68aa-4797-bd80-27e3ba931d20",
      "metadata": {
        "id": "31aa099a-68aa-4797-bd80-27e3ba931d20"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26d35dd5-c063-4fe7-bda6-a3e8b654f095",
      "metadata": {
        "id": "26d35dd5-c063-4fe7-bda6-a3e8b654f095"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d73b6a4-a5aa-467c-8cb6-bfdd3ab2b10c",
      "metadata": {
        "id": "2d73b6a4-a5aa-467c-8cb6-bfdd3ab2b10c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5944c90-22c6-4ae4-b347-68f57fb5c223",
      "metadata": {
        "id": "b5944c90-22c6-4ae4-b347-68f57fb5c223"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99162024-2041-477c-bcd4-61a78c5f9391",
      "metadata": {
        "id": "99162024-2041-477c-bcd4-61a78c5f9391"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec01fb5-7a72-489b-8fe7-ad1b2ba83726",
      "metadata": {
        "id": "bec01fb5-7a72-489b-8fe7-ad1b2ba83726"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d423c221-a913-42a0-aee7-064cc94f373e",
      "metadata": {
        "id": "d423c221-a913-42a0-aee7-064cc94f373e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e99bd8-ef2d-481f-a298-3cb2754a965a",
      "metadata": {
        "id": "f2e99bd8-ef2d-481f-a298-3cb2754a965a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5037807-429e-4f9d-a0ef-d1f72a43885e",
      "metadata": {
        "id": "d5037807-429e-4f9d-a0ef-d1f72a43885e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26bf7b2c-9ad9-4f65-984c-f0103ee2f333",
      "metadata": {
        "id": "26bf7b2c-9ad9-4f65-984c-f0103ee2f333"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c29643-2fdc-493d-89a1-8a70959c0fc5",
      "metadata": {
        "id": "a3c29643-2fdc-493d-89a1-8a70959c0fc5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af5f73b-1f05-49eb-92f8-8e4ad6d63a23",
      "metadata": {
        "id": "0af5f73b-1f05-49eb-92f8-8e4ad6d63a23"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}