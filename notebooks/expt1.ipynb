{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Set-Up"
      ],
      "metadata": {
        "id": "ovFCTTS_r9BZ"
      },
      "id": "ovFCTTS_r9BZ"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9ed7750a-cc36-4072-b2c0-284bf8f53473",
      "metadata": {
        "collapsed": true,
        "id": "9ed7750a-cc36-4072-b2c0-284bf8f53473"
      },
      "outputs": [],
      "source": [
        "# !pip install openai\n",
        "# !pip install dspy\n",
        "from openai import OpenAI\n",
        "import dspy\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fa2e36ed-f6af-4962-b69a-aee89f995f3e",
      "metadata": {
        "id": "fa2e36ed-f6af-4962-b69a-aee89f995f3e"
      },
      "outputs": [],
      "source": [
        "# Evaluate correctness of the answer - depends on format - will we pass two integers here as proposed by Vineet's logic?\n",
        "def evaluate_answer(model_answer, correct_answer):\n",
        "    if model_answer == \"A\" and correct_answer == 0:\n",
        "      return True\n",
        "    elif model_answer == \"B\" and correct_answer == 1:\n",
        "      return True\n",
        "    elif model_answer == \"C\" and correct_answer == 2:\n",
        "      return True\n",
        "    elif model_answer == \"D\" and correct_answer == 3:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "726a25f5-7d18-443c-a51f-d4b77bf3dee2",
      "metadata": {
        "id": "726a25f5-7d18-443c-a51f-d4b77bf3dee2"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "train_df=pd.read_csv('train_df.csv')\n",
        "train_df=train_df.fillna('#')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key='<add_your_key_here>'\n",
        "\n",
        "#Pull the OpenAI key from colab\n",
        "#from google.colab import userdata\n",
        "#openai_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "4TBUVUI8uv1U"
      },
      "id": "4TBUVUI8uv1U",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1df4dbd4-8e08-4235-bae4-9c07e91ca1c2",
      "metadata": {
        "id": "1df4dbd4-8e08-4235-bae4-9c07e91ca1c2"
      },
      "outputs": [],
      "source": [
        "lm = dspy.OpenAI(model='gpt-4o-mini', api_key=openai_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7a9ca327-5452-4a1d-9ffb-416c8c35756a",
      "metadata": {
        "id": "7a9ca327-5452-4a1d-9ffb-416c8c35756a"
      },
      "outputs": [],
      "source": [
        "dspy.settings.configure(lm=lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Questions"
      ],
      "metadata": {
        "id": "hgXI_rPTsQXX"
      },
      "id": "hgXI_rPTsQXX"
    },
    {
      "cell_type": "code",
      "source": [
        "#Question id=0 in logiqa train data set.  Use for debugging prompts.\n",
        "test_context = \"Some Cantonese don't like chili, so some southerners don't like chili.\"\n",
        "test_question = \"Which of the following can guarantee the above argument?\"\n",
        "test_choices = [\"Some Cantonese love chili.\", \"Some people who like peppers are southerners.\", \"All Cantonese are southerners.\", \"Some Cantonese like neither peppers nor sweets.\"]\n",
        "#test_answer_probabilities = [0.05,0.10,0.80,0.05]\n",
        "test_answer_letter_choice = 'C'\n",
        "test_correct_answer = 2\n",
        "\n",
        "#Note that these are Question id=1,2 in logiqa train data set.  Need to remove it from train data set?\n",
        "#Probabilities are simply made up, with the correct answer being the highest probability\n",
        "fewshot_example1_context = \"Continuous exposure to indoor fluorescent lights is beneficial to the health of hamsters with heart disease. One group of hamsters exposed to continuous exposure to fluorescent lights has an average lifespan that is 2.5% longer than another one of the same species but living in a black wall.\"\n",
        "fewshot_example1_question = \"Which of the following questions was the initial motivation for conducting the above experiment?\"\n",
        "fewshot_example1_choices = ['Can hospital light therapy be proved to promote patient recovery?', 'Which one lives longer, the hamster living under the light or the hamster living in the dark?', 'What kind of illness does the hamster have?', 'Do some hamsters need a period of darkness?']\n",
        "fewshot_example1_answer_probabilities = [0.75,0.10,0.02,0.13]\n",
        "fewshot_example1_answer_letter_choice = 'A'\n",
        "fewshot_example1_correct_answer = 0\n",
        "\n",
        "fewshot_example2_context = \"There is no doubt that minors should be prohibited from smoking. However, we cannot explicitly ban the use of automatic cigarette vending machines in order to prevent minors from smoking. This ban is just like setting up roadblocks on the road to prohibit driving without a license. These roadblocks naturally prohibit driving without a license, but also block more than 99% of licensed drivers.\"\n",
        "fewshot_example2_question = \"In order to evaluate the above argument, which of the following questions is the most important?\"\n",
        "fewshot_example2_choices = ['Does the proportion of underage smokers in the total number of smokers exceed 1%?', 'How much inconvenience does the ban on the use of automatic vending machines bring to adult cigarette buyers?', 'Whether the proportion of unlicensed drivers in the total number of drivers really does not exceed 1%.', 'Is the harm of minor smoking really as serious as the public thinks?']\n",
        "fewshot_example2_answer_probabilities = [0.19,0.67,0.09,0.05]\n",
        "fewshot_example2_answer_letter_choice = 'B'\n",
        "fewshot_example2_correct_answer = 1"
      ],
      "metadata": {
        "id": "vKjG7D3bsQoG"
      },
      "id": "vKjG7D3bsQoG",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d0b9c192-969b-45bc-8807-580e79451f9c",
      "metadata": {
        "id": "d0b9c192-969b-45bc-8807-580e79451f9c"
      },
      "source": [
        "### Prompts and LLM Query"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_choices(choices):\n",
        "    # Format the choices - using characters for now - maybe change to integers?\n",
        "    choices_text = '\\n'.join([f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)])\n",
        "    return choices_text\n",
        ""
      ],
      "metadata": {
        "id": "LyYPteDE2pUm"
      },
      "id": "LyYPteDE2pUm",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dc01b29b-cf52-477f-b349-2ab8642a142d",
      "metadata": {
        "id": "dc01b29b-cf52-477f-b349-2ab8642a142d"
      },
      "outputs": [],
      "source": [
        "def ask_llm_mc_question_1hop(context, question, choices, debug=False):\n",
        "    '''\n",
        "    Ask LLM to respond with correct answer and associated confidence (only for correct option)\n",
        "    '''\n",
        "    choices_text = format_choices(choices)\n",
        "\n",
        "    # Construct the prompt\n",
        "    if context!='#':\n",
        "        prompt = f\"\"\"\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Please choose the best answer by returning the letter (A, B, C, etc.) corresponding to the correct choice.  Just the letter, nothing else.\n",
        "        Provide the probability that your guess is correct. Give ONLY the probability, no\n",
        "        other words or explanation.\n",
        "        Finally return a tuple (choice, probability) where probability is between 0 and 1.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Please choose the best answer by returning the letter (A, B, C, etc.) corresponding to the correct choice.  Just the letter, nothing else.\n",
        "        Provide the probability that your guess is correct. Give ONLY the probability, no\n",
        "        other words or explanation.\n",
        "        Finally return a tuple (choice, probability) where probability is between 0 and 1.\n",
        "        \"\"\"\n",
        "\n",
        "    # Call the OpenAI API with the constructed prompt\n",
        "    response = lm(prompt, temperature=0, n=1)\n",
        "\n",
        "    response=response[0]\n",
        "    # Check if there's a comma in the response for splitting\n",
        "    if ',' in response:\n",
        "        answer, conf = response.split(',')\n",
        "    else:\n",
        "        answer = response\n",
        "        conf = 0.0  # or set to a default value like 0.0\n",
        "    # answer,conf = response[0].split(',')\n",
        "    if debug:\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {response}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(f\"Confidence: {conf}\")\n",
        "\n",
        "    return answer,conf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "776ee199-b4df-41b2-9e20-bac1894770dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "776ee199-b4df-41b2-9e20-bac1894770dd",
        "outputId": "07f24f75-bf59-4d25-c979-0c8fa1a1500f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \n",
            "        Context: Some Cantonese don't like chili, so some southerners don't like chili.\n",
            "        Question: Which of the following can guarantee the above argument?\n",
            "        Options: A. Some Cantonese love chili.\n",
            "B. Some people who like peppers are southerners.\n",
            "C. All Cantonese are southerners.\n",
            "D. Some Cantonese like neither peppers nor sweets.\n",
            "        Please choose the best answer by returning the letter (A, B, C, etc.) corresponding to the correct choice.  Just the letter, nothing else.\n",
            "        Provide the probability that your guess is correct. Give ONLY the probability, no\n",
            "        other words or explanation.\n",
            "        Finally return a tuple (choice, probability) where probability is between 0 and 1.\n",
            "        \n",
            "Response: (C, 0.9)\n",
            "Answer: (C\n",
            "Confidence:  0.9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('(C', ' 0.9)')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Get the LLM's answer\n",
        "ask_llm_mc_question_1hop(test_context, test_question, test_choices, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4198566a-5216-4068-809d-aa95ef5978dd",
      "metadata": {
        "id": "4198566a-5216-4068-809d-aa95ef5978dd"
      },
      "outputs": [],
      "source": [
        "def ask_llm_mc_question_1hop_withconf(context, question, choices, debug=False):\n",
        "    '''\n",
        "    Ask LLM to respond with  confidence for all  options\n",
        "    '''\n",
        "    choices_text = format_choices(choices)\n",
        "\n",
        "    # Construct the prompt\n",
        "\n",
        "    if context!='#':\n",
        "        prompt = f\"\"\"\n",
        "        Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"\n",
        "        Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the OpenAI API with the constructed prompt\n",
        "    response = lm(prompt, temperature=0, n=1)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the LLM's answer\n",
        "ask_llm_mc_question_1hop_withconf(test_context, test_question, test_choices, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87yzO86jxc8d",
        "outputId": "865be94a-dbdd-478c-f051-71eca59b08e6"
      },
      "id": "87yzO86jxc8d",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \n",
            "        Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
            "        Context: Some Cantonese don't like chili, so some southerners don't like chili.\n",
            "        Question: Which of the following can guarantee the above argument?\n",
            "        Options: A. Some Cantonese love chili.\n",
            "B. Some people who like peppers are southerners.\n",
            "C. All Cantonese are southerners.\n",
            "D. Some Cantonese like neither peppers nor sweets.\n",
            "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
            "        \n",
            "Response: ['A. 0  \\nB. 0  \\nC. 1  \\nD. 0  ']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A. 0  \\nB. 0  \\nC. 1  \\nD. 0  ']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "17ad6e63-57b5-4b5c-afd2-d405eb5bc147",
      "metadata": {
        "id": "17ad6e63-57b5-4b5c-afd2-d405eb5bc147"
      },
      "outputs": [],
      "source": [
        "def ask_llm_mc_question_1hop_withconf_fewshot(context, question, choices, fewshot=True, debug=False):\n",
        "    '''\n",
        "    Ask LLM to respond with  confidence for all  options, using few-shot prompting\n",
        "    '''\n",
        "\n",
        "    choices_text = format_choices(choices)\n",
        "    fewshot_example1_choices_text = format_choices(fewshot_example1_choices)\n",
        "    fewshot_example2_choices_text = format_choices(fewshot_example2_choices)\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt_task = \"Task: Provide the probability (between 0 and 1) for each option being correct, corresponding to options A, B, C, and D.  Return exactly 4 probabilities. No explanation is needed, only the probabilities.\"\n",
        "\n",
        "    if fewshot:\n",
        "        prompt_examples = f\"\"\"\n",
        "        Example 1:\n",
        "        Context: {fewshot_example1_context}\n",
        "        Question: {fewshot_example1_question}\n",
        "        Options: {fewshot_example1_choices_text}\n",
        "        Answer: {fewshot_example1_answer_probabilities}\n",
        "        Example 2:\n",
        "        Context: {fewshot_example2_context}\n",
        "        Question: {fewshot_example2_question}\n",
        "        Options: {fewshot_example2_choices_text}\n",
        "        Answer: {fewshot_example2_answer_probabilities}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt_examples = \"\"\n",
        "\n",
        "    if context!='#':\n",
        "        prompt_actual_question = f\"\"\"\n",
        "        Actual Question:\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt_actual_question = f\"\"\"\n",
        "        Actual Question:\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Answer:\n",
        "    \"\"\"\n",
        "    prompt = prompt_task + prompt_examples + prompt_actual_question\n",
        "\n",
        "    # Call the OpenAI API with the constructed prompt\n",
        "    response = lm(prompt, temperature=0, n=1)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the LLM's answer\n",
        "ask_llm_mc_question_1hop_withconf_fewshot(test_context, test_question, test_choices, fewshot=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRRBMomw1RfH",
        "outputId": "91d54843-b3ea-4df2-aa7b-2dc211278485"
      },
      "id": "HRRBMomw1RfH",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Task: Provide the probability (between 0 and 1) for each option being correct, corresponding to options A, B, C, and D.  Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
            "        Example 1:\n",
            "        Context: Continuous exposure to indoor fluorescent lights is beneficial to the health of hamsters with heart disease. One group of hamsters exposed to continuous exposure to fluorescent lights has an average lifespan that is 2.5% longer than another one of the same species but living in a black wall.\n",
            "        Question: Which of the following questions was the initial motivation for conducting the above experiment?\n",
            "        Options: A. Can hospital light therapy be proved to promote patient recovery?\n",
            "B. Which one lives longer, the hamster living under the light or the hamster living in the dark?\n",
            "C. What kind of illness does the hamster have?\n",
            "D. Do some hamsters need a period of darkness?\n",
            "        Answer: [0.75, 0.1, 0.02, 0.13]\n",
            "        Example 2:\n",
            "        Context: There is no doubt that minors should be prohibited from smoking. However, we cannot explicitly ban the use of automatic cigarette vending machines in order to prevent minors from smoking. This ban is just like setting up roadblocks on the road to prohibit driving without a license. These roadblocks naturally prohibit driving without a license, but also block more than 99% of licensed drivers.\n",
            "        Question: In order to evaluate the above argument, which of the following questions is the most important?\n",
            "        Options: A. Does the proportion of underage smokers in the total number of smokers exceed 1%?\n",
            "B. How much inconvenience does the ban on the use of automatic vending machines bring to adult cigarette buyers?\n",
            "C. Whether the proportion of unlicensed drivers in the total number of drivers really does not exceed 1%.\n",
            "D. Is the harm of minor smoking really as serious as the public thinks?\n",
            "        Answer: [0.19, 0.67, 0.09, 0.05]\n",
            "        \n",
            "        Actual Question:\n",
            "        Context: Some Cantonese don't like chili, so some southerners don't like chili.\n",
            "        Question: Which of the following can guarantee the above argument?\n",
            "        Options: A. Some Cantonese love chili.\n",
            "B. Some people who like peppers are southerners.\n",
            "C. All Cantonese are southerners.\n",
            "D. Some Cantonese like neither peppers nor sweets.\n",
            "        Answer:\n",
            "        \n",
            "Response: ['[0.05, 0.1, 0.8, 0.05]']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[0.05, 0.1, 0.8, 0.05]']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc7d97f-7ee0-487d-b87f-13b334a8ecbe",
      "metadata": {
        "id": "9bc7d97f-7ee0-487d-b87f-13b334a8ecbe"
      },
      "source": [
        "### Iterate over dataset and store results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317f34ec-1ba6-4c15-b765-31a1a611b5d2",
      "metadata": {
        "id": "317f34ec-1ba6-4c15-b765-31a1a611b5d2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import ast\n",
        "\n",
        "commonsense_qa_trainset_results_conf={}\n",
        "for idx,row in tqdm(train_df[train_df['ds_name']=='commonsense_qa'].iterrows()):\n",
        "    answer=ask_llm_mc_question_1hop_withconf(row['context'], row['question'], ast.literal_eval(row['options']))\n",
        "    commonsense_qa_trainset_results_conf[idx]=(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sgh-cZfOqLEH"
      },
      "id": "Sgh-cZfOqLEH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837b0a74-fd7f-40da-9da5-c8d02d2db07f",
      "metadata": {
        "id": "837b0a74-fd7f-40da-9da5-c8d02d2db07f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892eeb9f-2b5a-442a-9acb-f3eceef3bc27",
      "metadata": {
        "id": "892eeb9f-2b5a-442a-9acb-f3eceef3bc27"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e72feae-04b4-401f-b487-3b0f41ae717f",
      "metadata": {
        "id": "9e72feae-04b4-401f-b487-3b0f41ae717f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e4e3716d-cd2b-4644-a685-b92357e5a277",
      "metadata": {
        "id": "e4e3716d-cd2b-4644-a685-b92357e5a277"
      },
      "source": [
        "## TODO: Logit based expt\n",
        "see here : https://cookbook.openai.com/examples/using_logprobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3118a541-a677-497b-80f2-5c5816f30d0b",
      "metadata": {
        "id": "3118a541-a677-497b-80f2-5c5816f30d0b"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from math import exp\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3218cd3b-4288-4f0c-a34b-217731b6e8b0",
      "metadata": {
        "id": "3218cd3b-4288-4f0c-a34b-217731b6e8b0"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key='<add_key_here>')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda955f2-d063-4f92-852e-9c9d7d9e7d9d",
      "metadata": {
        "id": "dda955f2-d063-4f92-852e-9c9d7d9e7d9d"
      },
      "outputs": [],
      "source": [
        "def get_completion(\n",
        "    messages: list[dict[str, str]],\n",
        "    model: str = \"gpt-4o-mini\",\n",
        "    max_tokens=500,\n",
        "    temperature=0,\n",
        "    stop=None,\n",
        "    seed=123,\n",
        "    tools=None,\n",
        "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
        "    top_logprobs=None,\n",
        ") -> str:\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"stop\": stop,\n",
        "        \"seed\": seed,\n",
        "        \"logprobs\": logprobs,\n",
        "        \"top_logprobs\": top_logprobs,\n",
        "    }\n",
        "    if tools:\n",
        "        params[\"tools\"] = tools\n",
        "\n",
        "    completion = client.chat.completions.create(**params)\n",
        "    return completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98f74fb-85b1-4394-852b-ef370cd0cf2a",
      "metadata": {
        "id": "a98f74fb-85b1-4394-852b-ef370cd0cf2a"
      },
      "outputs": [],
      "source": [
        "QUERY_PROMPT = \"\"\"Provide the probability (between 0 and 1) for each option being correct, given the following:\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Options: {choices_text}\n",
        "        Return exactly 4 probabilities. No explanation is needed, only the probabilities.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57044607-0e95-4ae1-84f0-4ba67a4686c5",
      "metadata": {
        "id": "57044607-0e95-4ae1-84f0-4ba67a4686c5"
      },
      "outputs": [],
      "source": [
        "test_context = \"Some Cantonese don't like chili, so some southerners don't like chili.\"\n",
        "test_question = \"Which of the following can guarantee the above argument?\"\n",
        "test_choices = [\"Some Cantonese love chili.\", \"Some people who like peppers are southerners.\", \"All Cantonese are southerners.\", \"Some Cantonese like neither peppers nor sweets.\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c361cc5a-387e-47aa-ae72-25aaad8a03e8",
      "metadata": {
        "id": "c361cc5a-387e-47aa-ae72-25aaad8a03e8",
        "outputId": "92796e10-633d-4130-8b6c-4a29107088e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TopLogprob(token='[', bytes=[91], logprob=-0.01817628), TopLogprob(token='0', bytes=[48], logprob=-4.268176), TopLogprob(token=\"['\", bytes=[91, 39], logprob=-5.643176), TopLogprob(token='1', bytes=[49], logprob=-7.893176), TopLogprob(token='-', bytes=[45], logprob=-10.143176), TopLogprob(token='[\\n', bytes=[91, 10], logprob=-11.393176), TopLogprob(token='Prob', bytes=[80, 114, 111, 98], logprob=-11.393176), TopLogprob(token='Here', bytes=[72, 101, 114, 101], logprob=-11.643176), TopLogprob(token='Probability', bytes=[80, 114, 111, 98, 97, 98, 105, 108, 105, 116, 121], logprob=-12.393176), TopLogprob(token='Option', bytes=[79, 112, 116, 105, 111, 110], logprob=-12.518176)]\n"
          ]
        }
      ],
      "source": [
        "API_RESPONSE = get_completion(\n",
        "    [{\"role\": \"user\", \"content\": QUERY_PROMPT.format(context=test_context,question=test_question,choices_text=test_choices)}],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    logprobs=True,\n",
        "    top_logprobs=10,\n",
        ")\n",
        "top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n",
        "\n",
        "print(top_two_logprobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9113179-259a-465f-a665-3194a6f30fdf",
      "metadata": {
        "id": "a9113179-259a-465f-a665-3194a6f30fdf",
        "outputId": "3ce38b98-07c6-4878-e403-4d7d9ffcb72c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span style='color: cyan'>Output token 1:</span> [, <span style='color: darkorange'>logprobs:</span> -0.01817628, <span style='color: magenta'>linear probability:</span> 98.2%<br><span style='color: cyan'>Output token 2:</span> 0, <span style='color: darkorange'>logprobs:</span> -4.268176, <span style='color: magenta'>linear probability:</span> 1.4%<br><span style='color: cyan'>Output token 3:</span> [', <span style='color: darkorange'>logprobs:</span> -5.643176, <span style='color: magenta'>linear probability:</span> 0.35%<br><span style='color: cyan'>Output token 4:</span> 1, <span style='color: darkorange'>logprobs:</span> -7.893176, <span style='color: magenta'>linear probability:</span> 0.04%<br><span style='color: cyan'>Output token 5:</span> -, <span style='color: darkorange'>logprobs:</span> -10.143176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 6:</span> [\n",
              ", <span style='color: darkorange'>logprobs:</span> -11.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 7:</span> Prob, <span style='color: darkorange'>logprobs:</span> -11.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 8:</span> Here, <span style='color: darkorange'>logprobs:</span> -11.643176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 9:</span> Probability, <span style='color: darkorange'>logprobs:</span> -12.393176, <span style='color: magenta'>linear probability:</span> 0.0%<br><span style='color: cyan'>Output token 10:</span> Option, <span style='color: darkorange'>logprobs:</span> -12.518176, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "html_content = \"\"\n",
        "for i, logprob in enumerate(top_two_logprobs, start=1):\n",
        "    html_content += (\n",
        "        f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
        "        f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
        "        f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n",
        "    )\n",
        "display(HTML(html_content))\n",
        "print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdbd625-fc05-44ec-9cfc-852fc95fa84e",
      "metadata": {
        "id": "4fdbd625-fc05-44ec-9cfc-852fc95fa84e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd8f5f8-19e1-4a91-ab56-cbdbef54fb1b",
      "metadata": {
        "id": "5fd8f5f8-19e1-4a91-ab56-cbdbef54fb1b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1ae5c1-5d88-4aee-b0af-4df98b55b9df",
      "metadata": {
        "id": "ad1ae5c1-5d88-4aee-b0af-4df98b55b9df"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c5ecb6-9f45-4c26-b101-8284a5994978",
      "metadata": {
        "id": "27c5ecb6-9f45-4c26-b101-8284a5994978"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fcf1be-f50e-4303-af55-a95889031881",
      "metadata": {
        "id": "06fcf1be-f50e-4303-af55-a95889031881"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f09d22f-bd4c-4780-8182-45e4066f6fcf",
      "metadata": {
        "id": "3f09d22f-bd4c-4780-8182-45e4066f6fcf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfd1849-c1aa-4068-9332-b7885255c16f",
      "metadata": {
        "id": "1dfd1849-c1aa-4068-9332-b7885255c16f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbb47b5-5e52-45bc-935e-8bb5ce1b0090",
      "metadata": {
        "id": "1dbb47b5-5e52-45bc-935e-8bb5ce1b0090"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220cacb7-a187-47dd-95ce-f879f85328c6",
      "metadata": {
        "id": "220cacb7-a187-47dd-95ce-f879f85328c6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50804712-5fe2-4b4d-ae49-379f8a8e40e6",
      "metadata": {
        "id": "50804712-5fe2-4b4d-ae49-379f8a8e40e6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8419550-b762-4d79-83d6-45ee1dbcf76b",
      "metadata": {
        "id": "f8419550-b762-4d79-83d6-45ee1dbcf76b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ad8f14-6083-4eca-9d84-a253e5b35be0",
      "metadata": {
        "id": "49ad8f14-6083-4eca-9d84-a253e5b35be0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab35eea1-645a-4700-bb15-ba8281118000",
      "metadata": {
        "id": "ab35eea1-645a-4700-bb15-ba8281118000"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916905f8-e67b-4963-b499-7b0a82e5cf21",
      "metadata": {
        "id": "916905f8-e67b-4963-b499-7b0a82e5cf21"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71cdd9e2-56a8-44c9-806e-acd74d520c54",
      "metadata": {
        "id": "71cdd9e2-56a8-44c9-806e-acd74d520c54"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b37d82-c85b-4706-81a5-c88b6f6d83b1",
      "metadata": {
        "id": "13b37d82-c85b-4706-81a5-c88b6f6d83b1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0469c11-2021-4af8-a533-ef79406b408d",
      "metadata": {
        "id": "a0469c11-2021-4af8-a533-ef79406b408d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31aa099a-68aa-4797-bd80-27e3ba931d20",
      "metadata": {
        "id": "31aa099a-68aa-4797-bd80-27e3ba931d20"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26d35dd5-c063-4fe7-bda6-a3e8b654f095",
      "metadata": {
        "id": "26d35dd5-c063-4fe7-bda6-a3e8b654f095"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d73b6a4-a5aa-467c-8cb6-bfdd3ab2b10c",
      "metadata": {
        "id": "2d73b6a4-a5aa-467c-8cb6-bfdd3ab2b10c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5944c90-22c6-4ae4-b347-68f57fb5c223",
      "metadata": {
        "id": "b5944c90-22c6-4ae4-b347-68f57fb5c223"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99162024-2041-477c-bcd4-61a78c5f9391",
      "metadata": {
        "id": "99162024-2041-477c-bcd4-61a78c5f9391"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec01fb5-7a72-489b-8fe7-ad1b2ba83726",
      "metadata": {
        "id": "bec01fb5-7a72-489b-8fe7-ad1b2ba83726"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d423c221-a913-42a0-aee7-064cc94f373e",
      "metadata": {
        "id": "d423c221-a913-42a0-aee7-064cc94f373e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e99bd8-ef2d-481f-a298-3cb2754a965a",
      "metadata": {
        "id": "f2e99bd8-ef2d-481f-a298-3cb2754a965a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5037807-429e-4f9d-a0ef-d1f72a43885e",
      "metadata": {
        "id": "d5037807-429e-4f9d-a0ef-d1f72a43885e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26bf7b2c-9ad9-4f65-984c-f0103ee2f333",
      "metadata": {
        "id": "26bf7b2c-9ad9-4f65-984c-f0103ee2f333"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c29643-2fdc-493d-89a1-8a70959c0fc5",
      "metadata": {
        "id": "a3c29643-2fdc-493d-89a1-8a70959c0fc5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af5f73b-1f05-49eb-92f8-8e4ad6d63a23",
      "metadata": {
        "id": "0af5f73b-1f05-49eb-92f8-8e4ad6d63a23"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}